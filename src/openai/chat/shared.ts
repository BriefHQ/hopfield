import { openAIChatModelNames } from '../models.js';
import { z } from 'zod';

export const ChoiceIndex = z.number().nonnegative();

export const FunctionName = z
  .string()
  .refine((value) => /^[a-zA-Z0-9_]{1,64}$/.test(value), {
    message:
      'Function name can only contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.',
  });

export const FunctionCall = z
  .object({
    arguments: z.string(),
    name: FunctionName,
  })
  .describe(
    'The name and arguments of a function that should be called, as generated by the model.',
  );

export const MessageSystem = z.object({
  role: z.literal('system').describe('The role of the author of this message.'),
  content: z.string().describe('The contents of the system message.'),
});

export const MessageAssistant = z.object({
  role: z
    .literal('assistant')
    .describe('The role of the author of this message.'),
  content: z.string().describe('The contents of the assistant message.'),
});

export const MessageUser = z.object({
  role: z.literal('user').describe('The role of the author of this message.'),
  content: z.string().describe('The contents of the user message.'),
});

export const MessageAssistantFunctionCall = z.object({
  role: z
    .literal('assistant')
    .describe('The role of the author of this message.'),
  content: z.null(),
  function_call: FunctionCall,
});

export const MessageFunction = z.object({
  role: z
    .literal('function')
    .describe('The role of the author of this message.'),
  name: FunctionName,
  content: z.string().describe('The response from the function call.'),
});

export const Message = z.union([
  MessageAssistant,
  MessageAssistantFunctionCall,
  MessageUser,
  MessageFunction,
  MessageSystem,
]);

export type OpenAIChatMessage = z.infer<typeof Message>;

export const OpenAIChatBaseInput = z.object({
  /** ID of the model to use. */
  model: z.enum(openAIChatModelNames).describe('ID of the model to use.'),
  /** A list of messages comprising the conversation so far. */
  messages: z
    .array(Message)
    .describe('A list of messages comprising the conversation so far.'),
  /** Number between -2.0 and 2.0. Penalizes new tokens based on their existing frequency. */
  frequency_penalty: z
    .number()
    .min(-2)
    .max(2)
    .optional()
    .describe(
      'Number between -2.0 and 2.0. Penalizes new tokens based on their existing frequency.',
    ),
  /** Controls how the model responds to function calls. This cannot be overridden - use `hop.chat().functions([...])`. */
  function_call: z
    .never()
    .optional()
    .describe('Controls how the model responds to function calls.'),
  /** A list of functions the model may generate JSON inputs for. This cannot be overridden - use `hop.chat().functions([...])`. */
  functions: z
    .never()
    .optional()
    .describe('A list of functions the model may generate JSON inputs for.'),
  /** Modify the likelihood of specified tokens appearing in the completion. */
  logit_bias: z
    .record(z.number())
    .optional()
    .describe(
      'Modify the likelihood of specified tokens appearing in the completion.',
    ),
  /** The maximum number of tokens to generate in the chat completion. */
  max_tokens: z
    .number()
    .optional()
    .describe(
      'The maximum number of tokens to generate in the chat completion.',
    ),
  /** How many chat completion choices to generate for each input message. Defaults to 1. */
  n: z
    .number()
    .default(1)
    .describe(
      'How many chat completion choices to generate for each input message.',
    ),
  /** Number between -2.0 and 2.0. Penalizes new tokens based on whether they appear in the text. */
  presence_penalty: z
    .number()
    .min(-2)
    .max(2)
    .optional()
    .describe(
      'Number between -2.0 and 2.0. Penalizes new tokens based on whether they appear in the text.',
    ),
  /** Up to 4 sequences where the API will stop generating further tokens. */
  stop: z
    .union([z.array(z.string()), z.string(), z.null()])
    .optional()
    .describe(
      'Up to 4 sequences where the API will stop generating further tokens.',
    ),
  /** Indicates partial message deltas will *not* be sent. Always `false` - use `hop.chat().streaming()` to use streaming. */
  stream: z
    .literal(false)
    .default(false)
    .describe('If set, partial message deltas will be sent.'),
  /** What sampling temperature to use, between 0 and 2. */
  temperature: z
    .union([z.number().min(0).max(2), z.null()])
    .optional()
    .describe('What sampling temperature to use, between 0 and 2.'),
  /** An alternative to sampling with temperature, called nucleus sampling. */
  top_p: z
    .union([z.number().min(0).max(1), z.null()])
    .optional()
    .describe(
      'An alternative to sampling with temperature, called nucleus sampling.',
    ),
  /** A unique identifier representing your end-user. */
  user: z
    .string()
    .optional()
    .describe('A unique identifier representing your end-user.'),
});
