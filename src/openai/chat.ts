import { BaseHopfieldChat, type ChatStream } from '../chat.js';
import type { OpenAIFunctionSchema } from './function.js';
import {
  type DefaultOpenAIChatModelName,
  type OpenAIChatModelName,
  defaultOpenAIChatModelName,
  openAIChatModelNames,
} from './models.js';
import OpenAI from 'openai';
import { type ZodTypeAny, z } from 'zod';

/**
 * Enum for finish reasons
 * @enum {string}
 *
 * Enum Members:
 * - `stop`: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter
 * - `length`: Incomplete model output due to max_tokens parameter or token limit
 * - `function_call`: The model decided to call a function
 * - `content_filter`: Omitted content due to a flag from our content filters
 * - null: API response still in progress or incomplete
 */
const FinishReasonEnum = z
  .enum(['stop', 'length', 'function_call', 'content_filter'])
  .nullable()
  .describe('Enum for finish reasons.');

const MessageRoleEnum = z
  .enum(['system', 'user', 'assistant', 'function'])
  .describe('The role of the author of this message.');

const FunctionCall = z
  .object({
    arguments: z.string(),
    name: z.string(),
  })
  .describe(
    'The name and arguments of a function that should be called, as generated by the model.',
  );

const Message = z.object({
  role: MessageRoleEnum,
  content: z.string().nullable().describe('The contents of the message.'),
  function_call: FunctionCall.optional().describe(
    'The optional function call which was made in the message.',
  ),
});

export type OpenAIChatMessage = z.infer<typeof Message>;

const streamingChunkSchema = () => {
  const RoleDelta = z.object({
    role: z.enum(['assistant']),
  });

  const ContentDelta = z.object({
    content: z.string(),
  });

  const EmptyDelta = z.object({});

  const FunctionNameDelta = z.object({
    function_call: z.object({ name: z.string() }),
  });

  const FunctionArgsDelta = z.object({
    function_call: z.object({ arguments: z.string() }),
  });

  const ChoiceWithContentDelta = z.object({
    type: z.literal('CONTENT').default('CONTENT'),
    delta: ContentDelta,
    finish_reason: z.null(),
    index: z.number().nonnegative(),
  });

  const ChoiceWithRoleDelta = z.object({
    type: z.literal('ROLE').default('ROLE'),
    delta: RoleDelta,
    finish_reason: z.null(),
    index: z.number().nonnegative(),
  });

  const ChoiceWithStopReason = z.object({
    type: z.literal('STOP').default('STOP'),
    delta: EmptyDelta,
    finish_reason: z.literal('stop'),
    index: z.number(),
  });

  const ChoiceWithFunctionName = z.object({
    type: z.literal('FUNCTION_NAME').default('FUNCTION_NAME'),
    delta: FunctionNameDelta,
    finish_reason: z.null(),
    index: z.number().nonnegative(),
  });

  const ChoiceWithFunctionArgs = z.object({
    type: z.literal('FUNCTION_ARG').default('FUNCTION_ARG'),
    delta: FunctionArgsDelta,
    finish_reason: z.null(),
    index: z.number().nonnegative(),
  });

  const ChoiceWithFunctionCallReason = z.object({
    type: z.literal('FUNCTION_CALL').default('FUNCTION_CALL'),
    delta: EmptyDelta,
    finish_reason: z.literal('function_call'),
    index: z.number(),
  });

  return z.object({
    id: z.string(),
    choices: z.array(
      z.union([
        // role must come before content delta
        ChoiceWithRoleDelta,
        ChoiceWithContentDelta,
        ChoiceWithStopReason,
        ChoiceWithFunctionName,
        ChoiceWithFunctionArgs,
        ChoiceWithFunctionCallReason,
      ]),
    ),
    created: z.number(),
    model: z.string(),
    object: z.literal('chat.completion.chunk'),
  });
};

const nonStreamingChunkSchema = () => {
  const Choice = z.object({
    finish_reason: FinishReasonEnum,
    index: z.number().nonnegative(),
    message: Message,
  });

  const Usage = z.object({
    completion_tokens: z.number(),
    prompt_tokens: z.number(),
    total_tokens: z.number(),
  });

  return z.object({
    id: z.string(),
    choices: z.array(Choice),
    created: z.number(),
    model: z.string(),
    object: z.literal('chat.completion'),
    usage: Usage.optional(),
  });
};

const ChatCompletionChunk = streamingChunkSchema();

export type OpenAIChatCompletionChunk = z.infer<typeof ChatCompletionChunk>;

export type OpenAIChatCompletionChunkChoice =
  OpenAIChatCompletionChunk['choices'][number];

interface StreamingResult {
  [Symbol.asyncIterator](): AsyncIterableIterator<
    z.infer<typeof ChatCompletionChunk>
  >;
  streaming: true;
}

const ChatCompletion = nonStreamingChunkSchema();

interface NonStreamingResult extends z.infer<typeof ChatCompletion> {
  streaming: false;
}

export type OpenAIHopfieldChatSchemaProps<
  ModelName extends OpenAIChatModelName,
  ModelStream extends ChatStream,
  Functions extends readonly OpenAIFunctionSchema[],
> = {
  model?: ModelName;
  stream?: ModelStream;
  functions?: Functions;
};

export class OpenAIHopfieldChatSchema<
  ModelName extends OpenAIChatModelName = DefaultOpenAIChatModelName,
  ModelStream extends ChatStream = 'non-streaming',
  Functions extends readonly OpenAIFunctionSchema[] = [],
  FunctionParameters extends [ZodTypeAny, ...ZodTypeAny[]] | [] = [],
> extends BaseHopfieldChat<ModelName, ModelStream> {
  functions: Functions;
  functionSchema: FunctionParameters;

  constructor({
    model = defaultOpenAIChatModelName as ModelName,
    stream = 'non-streaming' as ModelStream,
    functions = [] as unknown as Functions,
  }: OpenAIHopfieldChatSchemaProps<ModelName, ModelStream, Functions>) {
    super({
      model,
      stream,
    });

    this.functions = functions;
    this.functionSchema = functions.map(
      (fn) => fn.parameters,
    ) as FunctionParameters;
  }

  get parameters() {
    const schema = z.object({
      messages: z
        .array(Message)
        .describe('A list of messages comprising the conversation so far.'),
      model: z
        .enum(openAIChatModelNames)
        .default(this.model)
        .describe('ID of the model to use.'),
      frequency_penalty: z
        .number()
        .min(-2)
        .max(2)
        .optional()
        .describe(
          'Number between -2.0 and 2.0. Penalizes new tokens based on their existing frequency.',
        ),
      function_call: z
        .enum(['none', 'auto'])
        .optional()
        .describe('Controls how the model responds to function calls.'),
      functions: z
        .tuple(this.functionSchema)
        .default(this.functions.map((fn) => fn.format()))
        .describe(
          'A list of functions the model may generate JSON inputs for.',
        ),
      logit_bias: z
        .record(z.number())
        .optional()
        .describe(
          'Modify the likelihood of specified tokens appearing in the completion.',
        ),
      max_tokens: z
        .number()
        .optional()
        .describe(
          'The maximum number of tokens to generate in the chat completion.',
        ),
      n: z
        .number()
        .optional()
        .describe(
          'How many chat completion choices to generate for each input message.',
        ),
      presence_penalty: z
        .number()
        .min(-2)
        .max(2)
        .optional()
        .describe(
          'Number between -2.0 and 2.0. Penalizes new tokens based on whether they appear in the text.',
        ),
      stop: z
        .union([z.array(z.string()), z.string(), z.null()])
        .optional()
        .describe(
          'Up to 4 sequences where the API will stop generating further tokens.',
        ),
      temperature: z
        .union([z.number().min(0).max(2), z.null()])
        .optional()
        .describe('What sampling temperature to use, between 0 and 2.'),
      top_p: z
        .union([z.number().min(0).max(1), z.null()])
        .optional()
        .describe(
          'An alternative to sampling with temperature, called nucleus sampling.',
        ),
      user: z
        .string()
        .optional()
        .describe('A unique identifier representing your end-user.'),
    });

    return this.stream === 'streaming'
      ? schema.extend({
          stream: z
            .literal(true)
            .default(true)
            .describe('If set, partial message deltas will be sent.'),
        })
      : schema;
  }

  get returnType() {
    return this.stream === 'streaming' ? ChatCompletionChunk : ChatCompletion;
  }

  static schema<
    ModelName extends OpenAIChatModelName,
    ModelStream extends ChatStream,
    Functions extends readonly OpenAIFunctionSchema[],
  >(opts: OpenAIHopfieldChatSchemaProps<ModelName, ModelStream, Functions>) {
    return new OpenAIHopfieldChatSchema(opts);
  }
}

export type OpenAIHopfieldChatProps<
  Provider,
  ModelName extends OpenAIChatModelName,
  ModelStream extends ChatStream,
  Functions extends readonly OpenAIFunctionSchema[],
> = {
  provider: Provider;
  model?: ModelName;
  stream?: ModelStream;
  functions?: Functions;
};

export class OpenAIHopfieldChat<
  Provider extends OpenAI,
  ModelName extends OpenAIChatModelName = DefaultOpenAIChatModelName,
  ModelStream extends ChatStream = 'non-streaming',
  Functions extends readonly OpenAIFunctionSchema[] = [],
> extends OpenAIHopfieldChatSchema<ModelName, ModelStream, Functions> {
  provider: Provider;

  constructor({
    provider,
    model = defaultOpenAIChatModelName as ModelName,
    stream = 'non-streaming' as ModelStream,
    functions = [] as unknown as Functions,
  }: OpenAIHopfieldChatProps<Provider, ModelName, ModelStream, Functions>) {
    super({
      model,
      stream,
      functions,
    });

    this.provider = provider;
  }

  async get<T extends this['stream']>(
    input: Omit<z.input<typeof this.parameters>, 'model'>,
  ): Promise<T extends 'streaming' ? StreamingResult : NonStreamingResult> {
    const parsedInput = await this.parameters.parseAsync(input);

    if (this.stream === 'streaming') {
      const response = await this.provider.chat.completions.create({
        ...parsedInput,
        stream: true,
      });

      const outputSchema = this.returnType;

      return {
        // define an async iterator
        [Symbol.asyncIterator]: async function* () {
          for await (const part of response) {
            console.log(JSON.stringify(part, null, 2));
            yield outputSchema.parseAsync(part);
          }
        },
        streaming: true,
      } as any;
    } else {
      const response = await this.provider.chat.completions.create(parsedInput);

      const parsed = await this.returnType.parseAsync(response);

      return {
        ...parsed,
        streaming: false,
      } as any;
    }
  }

  static create<
    Provider extends OpenAI,
    ModelName extends OpenAIChatModelName,
    ModelStream extends ChatStream,
    Functions extends readonly OpenAIFunctionSchema[],
  >(
    opts: OpenAIHopfieldChatProps<Provider, ModelName, ModelStream, Functions>,
  ) {
    return new OpenAIHopfieldChat(opts);
  }
}
